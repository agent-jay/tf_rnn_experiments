{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language model\n",
    "\n",
    "Adapted from [sherjilozair](https://github.com/sherjilozair/char-rnn-tensorflow)'s char-rnn tutorial\n",
    "\n",
    "Karpathy's canonical blog post, [The Unreasonable Effectiveness of Recurrent Neural Nets](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) claims:\n",
    "- x is the input to the language model. Is a character\n",
    "- y is the output of the RNN at every time step. Is a character\n",
    "- the previous hidden state encodes information from all the text the RNN has read up until that point. Question, what is the maximum length of the text encoded by the RNN. I'm thinking it's a static rnn with a max length window \n",
    "- If I'm right in my previous assumption, a datapoint represents a window of finite length over the text. The next datapoint will be this window shifted to the right by one character. Is this correct? Won't the training dataset be huge?\n",
    "\n",
    "Questions:\n",
    "- What is the architecture of a character level rnn\n",
    "- Can a dynamic rnn be used. is it even needed?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
