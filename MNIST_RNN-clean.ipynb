{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiying MNIST digits with an RNN \n",
    "\n",
    "Adapted from Aymeric Damien's [Tensorflow tutorials](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/recurrent_network.ipynb)\n",
    "\n",
    "The same digit classification problem as before, this time with more organized code as well as a few Tensorflow features- scoping and Tensorboard.\n",
    "\n",
    "References:\n",
    "- [Variable sharing](https://www.tensorflow.org/programmers_guide/variable_scope)\n",
    "- [Tensorboard basics](https://www.tensorflow.org/get_started/summaries_and_tensorboard)\n",
    "\t- Official tutorial on how to use summaries for later display on Tensorboard. Includes sample code for training and testing a convnet that shows best practices for scoping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import path\n",
    "cur_dir=path.os.getcwd()\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_iters = 10000\n",
    "batch_size = 10\n",
    "\n",
    "n_hidden=50\n",
    "\n",
    "n_steps= 28\n",
    "n_input= 28\n",
    "n_classes= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model and data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "x= tf.placeholder(tf.float32, (None, n_steps, n_input), 'input')\n",
    "y= tf.placeholder(tf.float32, (None, n_classes), 'label')\n",
    "\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    W= tf.Variable(tf.random_uniform((n_hidden, n_classes),0,1), name='W')\n",
    "    b= tf.Variable(tf.random_uniform((n_classes,1),0,1), name='b')\n",
    "    \n",
    "inps= tf.unstack(x, num=28, axis=1)\n",
    "lstm_cell= tf.contrib.rnn.LSTMCell(n_hidden)\n",
    "outputs, states= tf.contrib.rnn.static_rnn(lstm_cell, inps, dtype=tf.float32,   scope= 'rnnlm')\n",
    "pred=tf.matmul(outputs[-1],W) + b # batch_size*50, 50*10\n",
    "#Forward pass ends here. Fetch pred for test accuracy\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels= y, logits= pred))\n",
    "tf.summary.scalar('loss',loss)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "tf.summary.scalar('accuracy',accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.30122 0.0\n",
      "1000 0.194261 0.9\n",
      "2000 0.209981 0.9\n",
      "3000 0.027512 1.0\n",
      "4000 0.0834822 1.0\n",
      "5000 0.0079508 1.0\n",
      "6000 0.00940743 1.0\n",
      "7000 0.00399902 1.0\n",
      "8000 0.0100951 1.0\n",
      "9000 0.090911 0.9\n",
      "Now testing\n",
      "Accuracy on test set: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidjayam\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\sidjayam\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "def data_loader(data, batch_size):\n",
    "    inp,target= data.next_batch(10)\n",
    "    inp=np.reshape(inp, (batch_size, n_steps, n_input))\n",
    "    return inp, target\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(cur_dir+r'\\train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(cur_dir+r'\\test',sess.graph)\n",
    "    #sess.run(init_op) #If you're not using tf.InteractiveSession, then this is how you initialize global variables\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(training_iters):\n",
    "        inp, target= data_loader(mnist.train, batch_size)\n",
    "        _, l, acc, summary= sess.run([train_op,loss,accuracy, merged], feed_dict= {x:inp, y:target})\n",
    "        train_writer.add_summary(summary,i)\n",
    "        if i%1000==0:\n",
    "            print(i, l, acc)\n",
    "    \n",
    "    print(\"Now testing\")\n",
    "    accs=[]\n",
    "    while(mnist.test.epochs_completed==0):\n",
    "        data_loader(mnist.test, batch_size)\n",
    "        acc, summary= sess.run([accuracy,merged], feed_dict= {x:inp, y:target})\n",
    "        accs.append(acc)\n",
    "        i= len(accs)\n",
    "        test_writer.add_summary(summary,i)\n",
    "        if i%100==0:\n",
    "            print(i, acc)\n",
    "    print(\"Accuracy on test set:\", np.mean(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Graph Visualization\n",
    "\n",
    "Tensorboard makes extensive use of name_scopes. The better you're able to organize your code into name_scopes, the better the visualization\n",
    "\n",
    "![Tensorboard Graph Visualization](tensorboard-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Even when using namespaces, it seems unlikely (from where I'm standing now) that there is a perfect schema for organizing your code. There's too much criss-crossing of data to make a perfect abstraction. But one must strive...\n",
    "- Really need to understand scoping to make use of Tensorboard features but also useful nay, necessary for organizing large Tensorflow projects\n",
    "- A good way to run model on test data, is to use feed_dict to feed test data, but only fetch predictions. This essentially stops data flowing through the rest of the graph (loss, optimizer, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
