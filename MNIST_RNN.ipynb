{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiying MNIST digits with an RNN\n",
    "\n",
    "Adapted from Aymeric Damien's [Tensorflow tutorials](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/recurrent_network.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_iters = 10000\n",
    "batch_size = 10\n",
    "\n",
    "n_hidden=50\n",
    "\n",
    "n_steps= 28\n",
    "n_input= 28\n",
    "n_classes= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model and data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x= tf.placeholder(tf.float32, (None, n_steps, n_input), 'input')\n",
    "y= tf.placeholder(tf.float32, (None, n_classes))\n",
    "W= tf.Variable(tf.random_uniform((n_hidden, n_classes),0,1))\n",
    "b= tf.Variable(tf.random_uniform((n_classes,1),0,1))\n",
    "    \n",
    "inps= tf.unstack(x, num=28, axis=1)\n",
    "lstm_cell= tf.contrib.rnn.LSTMCell(n_hidden)\n",
    "outputs, states= tf.contrib.rnn.static_rnn(lstm_cell, inps, dtype=tf.float32)\n",
    "pred=tf.matmul(outputs[-1],W) + b # batch_size*50, 50*10\n",
    "\n",
    "loss= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels= y, logits= pred))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.31749 0.1\n",
      "1000 0.414572 0.8\n",
      "2000 0.412182 0.9\n",
      "3000 0.0647575 1.0\n",
      "4000 0.0786366 1.0\n",
      "5000 0.0489145 1.0\n",
      "6000 0.13115 0.9\n",
      "7000 0.292898 0.8\n",
      "8000 0.0823695 1.0\n",
      "9000 0.018918 1.0\n"
     ]
    }
   ],
   "source": [
    "def data_loader(data, batch_size):\n",
    "    inp,target= data.next_batch(10)\n",
    "    inp=np.reshape(inp, (batch_size, n_steps, n_input))\n",
    "    return inp, target\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op) #If you're not using tf.InteractiveSession, then this is how you initialize global variables\n",
    "    for i in range(training_iters):\n",
    "        inp, target= data_loader(mnist.train, batch_size)\n",
    "        _, l, acc= sess.run([train_op,loss,accuracy], feed_dict= {x:inp, y:target})\n",
    "        if i%1000==0:\n",
    "            print(i, l, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and Gotchas\n",
    "\n",
    "- scopes are DEFINITELY neccesary. Pretty simple reason actually. You might build a graph for training and testing separately, but need the weights, parameters to be shared in both. \n",
    "    - Figure out a strategy for running model on test data\n",
    "- tf.unstack\n",
    "    - Commonly used function. It chips a tensor along a certain axis, best to see an example\n",
    "- tf.global_variable_initializer functions differently in InteractiveSession and regular Sessions. It cannot simply be called, it needs to be fetched in the current Session\n",
    "- APIs for static and dynamic RNNs are different. Requires different format of inputs. Static needs a list of (batch,inputs) sized, where the length of this list is the maximum length of the sequence\n",
    "- An affine transform is just a linear transformation of the input\n",
    "- tf.nn.cross_entropy_softmax_logit ...\n",
    "- sigmoid vs softmax\n",
    "    - sigmoid used for 2 or more mutually exclusive classes\n",
    "    - softmax for predicting discrete classes, of which sigmoid is special case k=2\n",
    "- logit- unnormalized log probablity (wx+b for distribution over binary variables). Although it can be interpreted that way, and it makes things convenient from an optimization objective front, why is the output of an affine transform \"considered\" to be a log probability? \n",
    "- sigmoid function different from sigmoid activation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
